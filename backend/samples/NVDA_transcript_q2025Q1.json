{
    "symbol": "NVDA",
    "quarter": "2025Q1",
    "transcript": [
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Good afternoon. My name is Regina and I will be your conference operator today. At this time, I would like to welcome everyone to NVIDIA's First Quarter Earnings Call. All lines have been placed on mute to prevent any background noise. After the speaker's remarks, there will be a question-and-answer session. Thank you. Simona Jankowski, you may begin your conference.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Simona Jankowski",
            "title": "Moderator",
            "content": "Thank you. Good afternoon, everyone, and welcome to NVIDIA's conference call for the first quarter of fiscal 2025. With me today from NVIDIA are Jen-Hsun Huang, President and Chief Executive Officer, and Colette Kress, Executive Vice President and Chief Financial Officer. I'd like to remind you that our call is being webcast live on NVIDIA's Investor Relations website. The webcast will be available for replay until the conference call to discuss our financial results for the second quarter of fiscal 2025. The content of today's call is NVIDIA's property. It can't be reproduced or transcribed without our prior written consent. During this call, we may make forward-looking statements based on current expectations. These are subject to a number of significant risks and uncertainties and our actual results may differ materially. For a discussion of factors that could affect our future financial results and business, please refer to the disclosure in today's earnings release, our most recent Forms 10-K and 10-Q and the reports that we may file on Form 8-K with the Securities and Exchange Commission. All our statements are made as of today, May 22, 2024, based on information currently available to us. Except as required by law, we assume no obligation to update any such statements. During this call, we will discuss non-GAAP financial measures. You can find a reconciliation of these non-GAAP financial measures to GAAP financial measures in our CFO commentary, which is posted on our website. Let me highlight some upcoming events. On Sunday, June 2nd, ahead of the Computex Technology Trade Show in Taiwan, Jen-Hsun will deliver a keynote which will be held in-person in Taipei as well as streamed live. And on June 5th, we will present at the Bank of America Technology Conference in San Francisco. With that let me turn the call over to Colette.",
            "sentiment": "0.5"
        },
        {
            "speaker": "Colette Kress",
            "title": "CFO",
            "content": "Thanks, Simona. Q1 was another record quarter. Revenue of $26 billion was up 18% sequentially and up 262% year-on-year and well above our outlook of $24 billion. Starting with Data Center. Data Center revenue of $22.6 billion was a record, up 23% sequentially and up 427% year-on-year, driven by continued strong demand for the NVIDIA Hopper GPU computing platform. Compute revenue grew more than 5x and networking revenue more than 3x from last year. Strong sequential data center growth was driven by all customer types, led by enterprise and consumer internet companies. Large cloud providers continue to drive strong growth as they deploy and ramp NVIDIA AI infrastructure at scale and represented the mid-40s as a percentage of our Data Center revenue. Training and inferencing AI on NVIDIA CUDA is driving meaningful acceleration in cloud rental revenue growth, delivering an immediate and strong return on cloud provider's investment. For every $1 spent on NVIDIA AI infrastructure, cloud providers have an opportunity to earn $5 in GPU instant hosting revenue over four years. NVIDIA's rich software stack and ecosystem and tight integration with cloud providers makes it easy for end customers up and running on NVIDIA GPU instances in the public cloud. For cloud rental customers, NVIDIA GPUs offer the best time to train models, the lowest cost to train models and the lowest cost to inference large language models. For public cloud providers, NVIDIA brings customers to their cloud, driving revenue growth and returns on their infrastructure investments. Leading large language model companies such as OpenAI, Adept, Anthropic, Character.AI, Cohere, Databricks, DeepMind, Meta, Mistral, xAI, and many others are building on NVIDIA AI in the cloud. Enterprises drove strong sequential growth in Data Center this quarter. We supported Tesla's expansion of their training AI cluster to 35,000 H100 GPUs. Their use of NVIDIA AI infrastructure paved the way for the breakthrough performance of FSD Version 12, their latest autonomous driving software based on Vision. Video Transformers, while consuming significantly more computing, are enabling dramatically better autonomous driving capabilities and propelling significant growth for NVIDIA AI infrastructure across the automotive industry. We expect automotive to be our largest enterprise vertical within Data Center this year, driving a multibillion revenue opportunity across on-prem and cloud consumption. Consumer Internet companies are also a strong growth vertical. A big highlight this quarter was Meta's announcement of Llama 3, their latest large language model, which was trained on a cluster of 24,000 H100 GPUs. Llama 3 powers Meta AI, a new AI assistant available on Facebook, Instagram, WhatsApp and Messenger. Llama 3 is openly available and has kickstarted a wave of AI development across industries. As generative AI makes its way into more consumer internet applications, we expect to see continued growth opportunities as inference scales both with model complexity as well as with the number of users and number of queries per user, driving much more demand for AI compute. In our trailing four quarters, we estimate that inference drove about 40% of our Data Center revenue. Both training and inference are growing significantly. Large clusters like the ones built by Meta and Tesla are examples of the essential infrastructure for AI production, what we refer to as AI factories. These next-generation data centers host advanced full-stack accelerated computing platforms where the data comes in and intelligence comes out. In Q1, we worked with over 100 customers building AI factories ranging in size from hundreds to tens of thousands of GPUs, with some reaching 100,000 GPUs. From a geographic perspective, Data Center revenue continues to diversify as countries around the world invest in Sovereign AI. Sovereign AI refers to a nation's capabilities to produce artificial intelligence using its own infrastructure, data, workforce and business networks. Nations are building up domestic computing capacity through various models. Some are procuring and operating Sovereign AI clouds in collaboration with state-owned telecommunication providers or utilities. Others are sponsoring local cloud partners to provide a shared AI computing platform for public and private sector use. For example, Japan plans to invest more than $740 million in key digital infrastructure providers, including KDDI, Sakura Internet, and SoftBank to build out the nation's Sovereign AI infrastructure. France-based, Scaleway, a subsidiary of the Iliad Group, is building Europe's most powerful cloud native AI supercomputer. In Italy, Swisscom Group will build the nation's first and most powerful NVIDIA DGX-powered supercomputer to develop the first LLM natively trained in the Italian language. And in Singapore, the National Supercomputer Center is getting upgraded with NVIDIA Hopper GPUs, while Singtel is building NVIDIA's accelerated AI factories across Southeast Asia. NVIDIA's ability to offer end-to-end compute to networking technologies, full-stack software, AI expertise, and rich ecosystem of partners and customers allows Sovereign AI and regional cloud providers to jumpstart their country's AI ambitions. From nothing the previous year, we believe Sovereign AI revenue can approach the high single-digit billions this year. The importance of AI has caught the attention of every nation. We ramped new products designed specifically for China that don't require an export control license. Our Data Center revenue in China is down significantly from the level prior to the imposition of the new export control restrictions in October. We expect the market in China to remain very competitive going forward. From a product perspective, the vast majority of compute revenue was driven by our Hopper GPU architecture. Demand for Hopper during the quarter continues to increase. Thanks to CUDA algorithm innovations, we've been able to accelerate LLM inference on H100 by up to 3x, which can translate to a 3x cost reduction for serving popular models like Llama 3. We started sampling the H200 in Q1 and are currently in production with shipments on track for Q2. The first H200 system was delivered by Jen-Hsun to Sam Altman and the team at OpenAI and powered their amazing GPT-4o demos last week. H200 nearly doubles the inference performance of H100, delivering significant value for production deployments. For example, using Llama 3 with 700 billion parameters, a single NVIDIA HGX H200 server can deliver 24,000 tokens per second, supporting more than 2,400 users at the same time. That means for every $1 spent on NVIDIA HGX H200 servers at current prices per token, an API provider serving Llama 3 tokens can generate $7 in revenue over four years. With ongoing software optimizations, we continue to improve the performance of NVIDIA AI infrastructure for serving AI models. While supply for H100 proves, we are still constrained on H200. At the same time, Blackwell is in full production. We are working to bring up our system and cloud partners for global availability later this year. Demand for H200 and Blackwell is well ahead of supply and we expect demand may exceed supply well into next year. Grace Hopper Superchip is shipping in volume. Last week at the International Supercomputing Conference, we announced that nine new supercomputers worldwide are using Grace Hopper for a combined 200 exaflops of energy-efficient AI processing power delivered this year. These include the Alps Supercomputer at the Swiss National Supercomputing Center, the fastest AI supercomputer in Europe. Isambard-AI at the University of Bristol in the UK and JUPITER in the Julich Supercomputing Center in Germany. We are seeing an 80% attach rate of Grace Hopper in supercomputing due to its high energy efficiency and performance. We are also proud to see supercomputers powered with Grace Hopper take the number one, the number two, and the number three spots of the most energy-efficient supercomputers in the world. Strong networking year-on-year growth was driven by InfiniBand. We experienced a modest sequential decline, which was largely due to the timing of supply, with demand well ahead of what we were able to ship. We expect networking to return to sequential growth in Q2. In the first quarter, we started shipping our new Spectrum-X Ethernet networking solution optimized for AI from the ground up. It includes our Spectrum-4 switch, BlueField-3 DPU, and new software technologies to overcome the challenges of AI on Ethernet to deliver 1.6x higher networking performance for AI processing compared with traditional Ethernet. Spectrum-X is ramping in volume with multiple customers, including a massive 100,000 GPU cluster. Spectrum-X opens a brand-new market to NVIDIA networking and enables Ethernet only data centers to accommodate large-scale AI. We expect Spectrum-X to jump to a multibillion-dollar product line within a year. At GTC in March, we launched our next-generation AI factory platform, Blackwell. The Blackwell GPU architecture delivers up to 4x faster training and 30x faster inference than the H100 and enables real-time generative AI on trillion-parameter large language models. Blackwell is a giant leap with up to 25x lower TCO and energy consumption than Hopper. The Blackwell platform includes the fifth-generation NVLink with a multi-GPU spine and new InfiniBand and Ethernet switches, the X800 series designed for a trillion-parameter scale AI. Blackwell is designed to support data centers universally, from hyperscale to enterprise, training to inference, x86 to Grace CPUs, Ethernet to InfiniBand networking, and air cooling to liquid cooling. Blackwell will be available in over 100 OEM and ODM systems at launch, more than double the number of Hopper's launch and representing every major computer maker in the world. This will support fast and broad adoption across customer types, workloads and data center environments in the first year shipments. Blackwell time-to-market customers include Amazon, Google, Meta, Microsoft, OpenAI, Oracle, Tesla, and xAI. We announced a new software product with the introduction of NVIDIA Inference Microservices or NIM. NIM provides secure and performance-optimized containers powered by NVIDIA CUDA acceleration in network computing and inference software, including Triton Inference Server and TensorRT LLM with industry-standard APIs for a broad range of use cases, including large language models for text, speech, imaging, vision, robotics, genomics and digital biology. They enable developers to quickly build and deploy generative AI applications using leading models from NVIDIA, AI21, Adept, Cohere, Getty Images, and Shutterstock and open models from Google, Hugging Face, Meta, Microsoft, Mistral AI, Snowflake and Stability AI. NIMs will be offered as part of our NVIDIA AI enterprise software platform for production deployment in the cloud or on-prem. Moving to gaming and AI PCs. Gaming revenue of $2.65 billion was down 8% sequentially and up 18% year-on-year, consistent with our outlook for a seasonal decline. The GeForce RTX Super GPUs market reception is strong and end demand and channel inventory remained healthy across the product range. From the very start of our AI journey, we equipped GeForce RTX GPUs with CUDA Tensor Cores. Now with over 100 million of an installed base, GeForce RTX GPUs are perfect for gamers, creators, AI enthusiasts and offer unmatched performance for running generative AI applications on PCs. NVIDIA has full technology stack for deploying and running fast and efficient generative AI inference on GeForce RTX PCs. TensorRT LLM now accelerates Microsoft's Phi-3-Mini model and Google's Gemma 2B and 7B models as well as popular AI frameworks, including LangChain and LlamaIndex. Yesterday, NVIDIA and Microsoft announced AI performance optimizations for Windows to help run LLMs up to 3x faster on NVIDIA GeForce RTX AI PCs. And top game developers, including NetEase Games, Tencent and Ubisoft are embracing NVIDIA Avatar Character Engine to create lifelike avatars to transform interactions between gamers and non-playable characters. Moving to ProVis. Revenue of $427 million was down 8% sequentially and up 45% year-on-year. We believe generative AI and Omniverse industrial digitalization will drive the next wave of professional visualization growth. At GTC, we announced new Omniverse Cloud APIs to enable developers to integrate Omniverse industrial digital twin and simulation technologies into their applications. Some of the world's largest industrial software makers are adopting these APIs, including ANSYS, Cadence, 3DEXCITE at Dassault Systemes, Brand and Siemens. And developers can use them to stream industrial digital twins with spatial computing devices such as Apple Vision Pro. Omniverse Cloud APIs will be available on Microsoft Azure later this year. Companies are using Omniverse to digitalize their workflows. Omniverse power digital twins enable Wistron, one of our manufacturing partners, to reduce end-to-end production cycle times by 50% and defect rates by 40%. And BYD, the world's largest electric vehicle maker, is adopting Omniverse for virtual factory planning and retail configurations. Moving to automotive. Revenue was $329 million, up 17% sequentially and up 11% year-on-year. Sequential growth was driven by the ramp of AI cockpit solutions with global OEM customers and strength in our self-driving platforms. Year-on-year growth was driven primarily by self-driving. We supported Xiaomi in the successful launch of its first electric vehicle, the SU7 sedan built on the NVIDIA DRIVE Orin, our AI car computer for software-defined AV fleets. We also announced a number of new design wins on NVIDIA DRIVE Thor, the successor to Orin, powered by the new NVIDIA Blackwell architecture with several leading EV makers, including BYD, XPeng, GAC's Aion Hyper and Neuro. DRIVE Thor is slated for production vehicles starting next year. Okay. Moving to the rest of the P&L. GAAP gross margin expanded sequentially to 78.4% and non-GAAP gross margins to 78.9% on lower inventory targets. As noted last quarter, both Q4 and Q1 benefited from favorable component costs. Sequentially, GAAP operating expenses were up 10% and non-GAAP operating expenses were up 13%, primarily reflecting higher compensation-related costs and increased compute and infrastructure investments. In Q1, we returned $7.8 billion to shareholders in the form of share repurchases and cash dividends. Today, we announced a 10-for-1 split of our shares with June 10th as the first day of trading on a split-adjusted basis. We are also increasing our dividend by 150%. Let me turn to the outlook for the second quarter. Total revenue is expected to be $28 billion, plus or minus 2%. We expect sequential growth in all market platforms. GAAP and non-GAAP gross margins are expected to be 74.8% and 75.5%, respectively, plus or minus 50 basis points, consistent with our discussion last quarter. For the full year, we expect gross margins to be in the mid-70s percent range. GAAP and non-GAAP operating expenses are expected to be approximately $4 billion and $2.8 billion, respectively. Full year OpEx is expected to grow in the low 40% range. GAAP and non-GAAP other income and expenses are expected to be an income of approximately, excuse me, approximately $300 million, excluding gains and losses from nonaffiliated investments. GAAP and non-GAAP tax rates are expected to be 17%, plus or minus 1%, excluding any discrete items. Further financial details are included in the CFO commentary and other information available on our IR website. I would like to now turn it over to Jen-Hsun as he would like to make a few comments.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "Thank you, Colette. The industry is undergoing a significant transformation. Before we move into the question and answer session, I want to highlight the significance of this change. We are entering a new industrial revolution. Companies and nations are collaborating with NVIDIA to transition the trillion-dollar landscape of traditional data centers to accelerated computing and establish a new kind of data center, which we refer to as AI factories, to produce the new commodity\u2014artificial intelligence. AI is set to enhance productivity across virtually every sector while enabling companies to be more cost-effective and energy efficient, all while unlocking new revenue streams. Cloud service providers were the pioneers in generative AI, utilizing NVIDIA to optimize workloads and reduce costs and energy consumption. The tokens generated by NVIDIA Hopper contribute to the revenue of their AI services. Additionally, NVIDIA cloud instances draw in rental clients from our extensive ecosystem of developers. The growing demand for generative AI training and inference on the Hopper platform is driving our Data Center expansion. As models evolve to be multimodal, we see continued scaling in training, encompassing text, speech, images, video, and 3D, alongside reasoning and planning capabilities. Our inference workloads are also experiencing remarkable growth. With generative AI, inference\u2014now centered around rapid token generation on a massive scale\u2014has become highly intricate. Generative AI is catalyzing a comprehensive shift in computing platforms, fundamentally altering all computer interactions. We are transitioning from a model focused on information retrieval to one that emphasizes generating answers and skills. AI will grasp context and our intentions, possess knowledge, engage in reasoning, planning, and execute tasks. We are revolutionizing how computing functions and the capabilities of computers, moving from general-purpose CPUs to GPU-accelerated systems, from instruction-based software to models that understand intentions, from information retrieval to skill performance, and at the industry level, from software generation to token manufacturing and digital intelligence creation. Token generation will spearhead a multi-year development of AI factories. Beyond cloud service providers, generative AI has now reached consumer Internet firms and enterprises, including sectors like automotive, healthcare, and Sovereign AI, creating several multi-billion-dollar markets. The Blackwell platform is fully operational and serves as the foundation for generative AI on a trillion-parameter scale. Our integration of Grace CPU, Blackwell GPUs, NVLink, Quantum, Spectrum, and high-speed interconnects, along with a robust ecosystem of software and partners, allows us to provide a more comprehensive solution for AI factories than previous generations. Spectrum-X introduces a new market opportunity for implementing large-scale AI in Ethernet-only data centers. Additionally, NVIDIA NIMs, our new software offering, delivers enterprise-level optimized generative AI compatible with CUDA across a wide range of environments\u2014from cloud to on-premises data centers to RTX AI PCs, facilitated through our extensive network of ecosystem partners. With advancements ranging from Blackwell to Spectrum-X to NIMs, we are prepared for the forthcoming wave of growth. Thank you.",
            "sentiment": "1.0"
        },
        {
            "speaker": "Simona Jankowski",
            "title": "Moderator",
            "content": "Thank you, Jen-Hsun. We will now open the call for questions. Operator, could you please poll for questions?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Your first question comes from the line of Stacy Rasgon with Bernstein. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Stacy Rasgon",
            "title": "Analyst",
            "content": "Hi, everyone. Thank you for taking my questions. My first question is about the Blackwell comment regarding its full production status. What does this indicate about shipment and delivery timelines now that the product seems to be in production rather than sampling? What does that mean for when customers will actually receive it?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "We will be shipping. Well, we've been in production for a little bit of time. But our production shipments will start in Q2 and ramp in Q3, and customers should have data centers stood up in Q4.",
            "sentiment": "0.8"
        },
        {
            "speaker": "Stacy Rasgon",
            "title": "Analyst",
            "content": "Got it. So this year, we will see Blackwell revenue, it sounds like?",
            "sentiment": "0.5"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "We will see a lot of Blackwell revenue this year.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Timothy Arcuri with UBS. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Timothy Arcuri",
            "title": "Analyst",
            "content": "Thanks a lot. I wanted to ask, Jen-Hsun, about the deployment of Blackwell versus Hopper just between the systems nature and all the demand for GB that you have. How does the deployment of this stuff differ from Hopper? I guess I ask because liquid cooling at scale hasn't been done before, and there's some engineering challenges both at the node level and within the data center. So do these complexities sort of elongate the transition? And how do you sort of think about how that's all going? Thanks.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "Yes. Blackwell comes in many configurations. Blackwell is a platform, not a GPU. And the platform includes support for air cooled, liquid cooled, x86 and Grace, InfiniBand, now Spectrum-X and very large NVLink domain that I demonstrated at GTC, that I showed at GTC. And so for some customers, they will ramp into their existing installed base of data centers that are already shipping Hoppers. They will easily transition from H100 to H200 to B100. And so Blackwell systems have been designed to be backwards compatible, if you will, electrically, mechanically. And of course, the software stack that runs on Hopper will run fantastically on Blackwell. We also have been priming the pump, if you will, with the entire ecosystem, getting them ready for liquid cooling. We've been talking to the ecosystem about Blackwell for quite some time. And the CSPs, the data centers, the ODMs, the system makers, our supply chain beyond them, the cooling supply chain base, data center supply chain base, no one is going to be surprised with Blackwell coming and the capabilities that we would like to deliver with Grace Blackwell 200. GB200 is going to be exceptional.",
            "sentiment": "0.7"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Vivek Arya with Bank of America Securities. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Vivek Arya",
            "title": "Analyst",
            "content": "Thanks for taking my question. Jensen, how are you ensuring that there is enough utilization of your products and that there isn't a pull-ahead or holding behavior because of tight supply, competition or other factors? Basically, what checks have you built in the system to give us confidence that monetization is keeping pace with your really very strong shipment growth?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "The demand for GPUs across all data centers is extremely high. We're in a constant race to keep up. Applications like ChatGPT, GPT-4, and others are utilizing every available GPU. There are also tens of thousands of generative AI startups across various fields, from multimedia to digital biology, all pushing for GPU access. Clients are pressuring us to deliver systems quickly, and we have not yet accounted for all the Sovereign AIs looking to train their own regional models. Overall, the demand vastly exceeds our supply.  In the long run, we are completely rethinking computer design, marking a significant platform shift that is likely more impactful than past shifts. The computer is evolving from being merely instruction-driven to being intention-understanding, enabling it to process and reason based on user intent. This shift means computers will no longer just retrieve files but instead generate contextually relevant, intelligent responses. This transformation will affect computing stacks worldwide, including the PC segment. What people are seeing now is only the beginning of our work in collaboration with startups, large companies, and developers globally; the future is set to be extraordinary.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Joe Moore with Morgan Stanley. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Joseph Moore",
            "title": "Analyst",
            "content": "Great. Thank you. I understand what you just said about how strong demand is. You have a lot of demand for H200 and for Blackwell products. Do you anticipate any kind of pause with Hopper and H100 as you sort of migrate to those products? Will people wait for those new products, which would be a good product to have? Or do you think there's enough demand for H100 to sustain growth?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "We observe rising demand for Hopper this quarter. We anticipate that demand will exceed supply for a while as we transition to H200 and Blackwell. Companies are eager to get their infrastructure up and running because it's a way for them to save and earn money, and they want to do that as quickly as possible.",
            "sentiment": "0.6"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Toshiya Hari with Goldman Sachs. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Toshiya Hari",
            "title": "Analyst",
            "content": "Hi. Thank you so much for taking the question. Jensen, I wanted to ask about competition. I think many of your cloud customers have announced new or updates to their existing internal programs, right, in parallel to what they're working on with you guys. To what extent did you consider them as competitors, medium to long term? And in your view, do you think they're limited to addressing most internal workloads or could they be broader in what they address going forward? Thank you.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "We're different in several ways. First, NVIDIA's accelerated computing architecture allows customers to process every aspect of their pipeline from unstructured data processing to prepare it for training, to structured data processing, data frame processing like SQL to prepare for training, to training to inference. And as I was mentioning in my remarks, that inference has really fundamentally changed, it's now generation. It's not trying to just detect the cat, which was plenty hard in itself, but it has to generate every pixel of a cat. And so the generation process is a fundamentally different processing architecture. And it's one of the reasons why TensorRT LLM was so well received. We improved the performance in using the same chips on our architecture by a factor of three. That kind of tells you something about the richness of our architecture and the richness of our software. So one, you could use NVIDIA for everything, from computer vision to image processing, the computer graphics to all modalities of computing. And as the world is now suffering from computing cost and computing energy inflation because general-purpose computing has run its course, accelerated computing is really the sustainable way of going forward. So accelerated computing is how you're going to save money in computing, is how you're going to save energy in computing. And so the versatility of our platform results in the lowest TCO for their data center. Second, we're in every cloud. And so for developers that are looking for a platform to develop on, starting with NVIDIA is always a great choice. And we're on-prem, we're in the cloud. We're in computers of any size and shape. We're practically everywhere. And so that's the second reason. The third reason has to do with the fact that we build AI factories. And this is becoming more apparent to people that AI is not a chip problem only. It starts, of course, with very good chips and we build a whole bunch of chips for our AI factories, but it's a systems problem. In fact, even AI is now a systems problem. It's not just one large language model. It's a complex system of a whole bunch of large language models that are working together. And so the fact that NVIDIA builds this system causes us to optimize all of our chips to work together as a system, to be able to have software that operates as a system, and to be able to optimize across the system. And just to put it in perspective in simple numbers, if you had a $5 billion infrastructure and you improved the performance by a factor of two, which we routinely do, when you improve the infrastructure by a factor of two, the value too is $5 billion. All the chips in that data center doesn't pay for it. And so the value of it is really quite extraordinary. And this is the reason why today, performance matters everything. This is at a time when the highest performance is also the lowest cost because the infrastructure cost of carrying all of these chips cost a lot of money. And it takes a lot of money to fund the data center, to operate the data center, the people that goes along with it, the power that goes along with it, the real estate that goes along with it, and all of it adds up. And so the highest performance is also the lowest TCO.",
            "sentiment": "0.8"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Matt Ramsay with TD Cowen. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Matthew Ramsay",
            "title": "Analyst",
            "content": "Thank you very much. Good afternoon, everyone. Jensen, I've spent my entire career in the data center industry, and I\u2019ve never witnessed the rapid pace at which you are introducing new platforms along with significant performance improvements, like 5x in training and even up to 30x in inference as mentioned at GTC. It's incredible to observe, but it also creates an interesting situation where the current generation of products that your customers are investing billions in will quickly fall behind your new offerings, much faster than the typical depreciation cycle of those products. I would appreciate it if you could share your insights on how you see this evolving with your customers. As you transition to Blackwell, there will be a substantial installed base, already software compatible, but it will consist of products that are far less capable than your new generation. I'm curious to hear what you anticipate regarding customer reactions during this transition. Thank you.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "I appreciate the opportunity to speak. I want to make a few points. There's a significant difference in perspective when you're 5% into a build-out compared to being 95% into it. Since we're only at 5%, our focus is on building as quickly as possible. When Blackwell arrives, it will be fantastic, and after that, we have more Blackwell releases coming. We're operating on a one-year cycle, as we've communicated previously. Customers are still in the early stages of their build-outs and need to keep progressing. Numerous challenges will arise, and they need to approach them by averaging out performance over time, which is a sensible strategy. They need to generate revenue and save costs now, and time holds immense value for them. For instance, having the capability to rapidly establish a data center and minimize time to train is crucial. The next company to reach a significant milestone will get to unveil an innovative AI solution, while the subsequent one will present something slightly improved. The choice lies between being the leader in groundbreaking AI or settling for minimal improvements. This competitive race is essential in technology, as it is vital for companies to establish trust in industry leadership and believe in the ongoing enhancement of the platforms they choose. Leadership and time to train are critical factors. A mere three-month difference in starting a project can be transformative. This is why we are urgently deploying Hopper systems; the next breakthrough is just ahead.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Mark Lipacis with Evercore ISI. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Mark Lipacis",
            "title": "Analyst",
            "content": "Hi. Thanks for taking my question. Jensen, in the past, you've made the observation that general-purpose computing ecosystems typically dominated each computing era. And I believe the argument was that they could adapt to different workloads, get higher utilization, drive costs of compute cycle down. And this is a motivation for why you were driving to a general-purpose GPU CUDA ecosystem for accelerated computing. And if I mischaracterized that observation, please do let me know. So the question is, given that the workloads that are driving demand for your solutions are being driven by neural network training and inferencing, which on the surface seem like a limited number of workloads, then it might also seem to lend themselves to custom solutions. And so then the question is about does the general purpose computing framework become more at risk or is there enough variability or a rapid enough evolution on these workloads that support that historical general purpose framework? Thank you.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "NVIDIA's accelerated computing is adaptable, but I wouldn\u2019t classify it as general-purpose. For instance, it's not suited for running spreadsheets, which are meant for general-purpose computing. The control loop of an operating system isn\u2019t ideal for general-purpose computing or accelerated computing. I would describe our technology as versatile, capable of handling a wide range of applications over the years, which share many similarities despite some deep differences. All applications that we work with can run in parallel and are heavily threaded. A small portion of the code can account for the majority of the runtime, which illustrates key characteristics of accelerated computing. The flexibility of our platform and our approach to designing entire systems are reasons why the number of startups discussed in these calls has grown considerably over the past decade. Many of these startups faced challenges due to the fragility of their architectures when generative AI emerged or as new models appeared. Now, large language models that require memory for contextual understanding are gaining importance, emphasizing the significance of the Grace memory. Each advancement in generative AI highlights the need for solutions that cater to a broader range rather than just focusing on a single model. Software will continue to evolve and improve, and we believe in the scaling potential of these models, estimating growth by a million times in the upcoming years. We are prepared for this evolution, and the adaptability of our platform is crucial. If a system is too rigid and narrowly defined, it might as well be an FPGA or an ASIC, which aren\u2019t equivalent to a true computer.",
            "sentiment": "0.8"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Blayne Curtis with Jefferies. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Blayne Curtis",
            "title": "Analyst",
            "content": "Thanks for taking my question. Actually kind of curious, I mean, being supply constrained, how do you think about, I mean, you came out with a product for China, H20. I'm assuming there'd be a ton of demand for it, but obviously, you're trying to serve your customers with the other Hopper products. Just kind of curious how you're thinking about that in the second half. You could elaborate any impact, what you're thinking for sales as well as gross margin.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "Well, we have customers that we honor and we do our best for every customer. It is the case that our business in China is substantially lower than the levels of the past. And it's a lot more competitive in China now because of the limitations on our technology. And so those matters are true. However, we continue to do our best to serve the customers in the markets there and to the best of our ability, we'll do our best. But I think overall, the comments that we made about demand outstripping supply is for the entire market and particularly so for H200 and Blackwell towards the end of the year.",
            "sentiment": "0.3"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line of Srini Pajjuri with Raymond James. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Srini Pajjuri",
            "title": "Analyst",
            "content": "Thank you. Jensen, actually more of a clarification on what you said. GB 200 systems, it looks like there is a significant demand for systems. Historically, I think you've sold a lot of HGX boards and some GPUs and the systems business was relatively small. So I'm just curious, why is it that now you are seeing such a strong demand for systems going forward? Is it just the TCO or is it something else or is it just the architecture? Thank you.",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "Yes. I appreciate that. In fact, the way we sell GB200 is the same. We disaggregate all of the components that make sense and we integrate it into computer makers. We have 100 different computer system configurations that are coming this year for Blackwell. And that is off the charts. Hopper, frankly, had only half, but that's at its peak. It started out with way less than that even. And so you're going to see liquid cooled version, air cooled version, x86 visions, Grace versions, so on and so forth. There's a whole bunch of systems that are being designed. And they're offered from all of our ecosystem of great partners. Nothing has really changed. Now of course, the Blackwell platform has expanded our offering tremendously. The integration of CPUs and the much more compressed density of computing, liquid cooling is going to save data centers a lot of money in provisioning power and not to mention to be more energy efficient. And so it's a much better solution. It's more expansive, meaning that we offer a lot more components of a data center and everybody wins. The data center gets much higher performance, networking from networking switches, networking. Of course, NICs, we have Ethernet now so that we can bring NVIDIA AI to a large-scale NVIDIA AI to customers who only operate only know how to operate Ethernet because of the ecosystem that they have. And so Blackwell is much more expansive. We have a lot more to offer our customers this generation around.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our next question will come from the line William Stein with Truist Securities. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "William Stein",
            "title": "Analyst",
            "content": "Great. Thanks for taking my question. Jensen, at some point, NVIDIA decided that while there are reasonably good CPUs available for data center operations, your ARM-based Grace CPU provides some real advantage that made that technology worth delivering to customers, perhaps related to cost or power consumption or technical synergies between Grace and Hopper, Grace and Blackwell. Can you address whether there could be a similar dynamic that might emerge on the client side, whereby while there are very good solutions, you've highlighted that Intel and AMD are very good partners and deliver great products in x86, but there might be some, especially in emerging AI workloads, some advantage that NVIDIA can deliver that others have more of a challenge?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "Well, you mentioned some really good reasons. It is true that for many of the applications, our partnership with x86 partners are really terrific and we build excellent systems together. But Grace allows us to do something that isn't possible with the configuration, the system configuration today. The memory system between Grace and Hopper are coherent and connected. The interconnect between the two chips, calling it two chips is almost weird because it's like a superchip. The two of them are connected with this interface that's like a terabytes per second. It's off the charts. And the memory that's used by Grace is LPDDR. It's the first data center-grade low-power memory. And so we save a lot of power on every single node. And then finally, because of the architecture, because we can create our own architecture with the entire system now, we could create something that has a really large NVLink domain, which is vitally important to the next-generation large language models for inferencing. And so you saw that GB200 has a 72-node NVLink domain. That's like 72 Blackwells connected together into one giant GPU. And so we needed Grace Blackwells to be able to do that. And so there are architectural reasons, there are software programming reasons and then there are system reasons that are essential for us to build them that way. And so if we see opportunities like that, we'll explore it. And today, as you saw at the build yesterday, which I thought was really excellent, Satya announced the next-generation PCs, Copilot+ PC, which runs fantastically on NVIDIA's RTX GPUs that are shipping in laptops. But it also supports ARM beautifully. And so it opens up opportunities for system innovation even for PCs.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "Our last question will come from the line of C.J. Muse with Cantor Fitzgerald. Please go ahead.",
            "sentiment": "0.0"
        },
        {
            "speaker": "C.J. Muse",
            "title": "Analyst",
            "content": "Good afternoon. Thank you for taking the question. I guess, Jensen, a bit of a longer-term question. I know Blackwell hasn't even launched yet, but obviously, investors are forward-looking and amidst rising potential competition from GPUs and custom ASICs, how are you thinking about NVIDIA's pace of innovation and your million-fold scaling over the last decade, truly impressive. CUDA, Varsity, Precision, Grace, Cohere and Connectivity. When you look forward, what frictions need to be solved in the coming decade? And I guess, maybe more importantly, what are you willing to share with us today?",
            "sentiment": "0.0"
        },
        {
            "speaker": "Jen-Hsun Huang",
            "title": "CEO",
            "content": "I can share that after Blackwell, we have another chip coming. We're on an annual schedule, and you can expect us to introduce new networking technology rapidly. We're launching Spectrum-X for Ethernet, and we are fully committed to Ethernet with an exciting roadmap ahead. We have a strong network of partners, including Dell, which is bringing Spectrum-X to market. Additionally, we have a robust ecosystem of customers and partners ready to promote our complete AI factory architecture. For companies seeking top performance, we offer InfiniBand computing fabric, which started as a computing fabric and has evolved into an excellent network. While Ethernet serves as a network, Spectrum-X will enhance it as a computing fabric. We are dedicated to advancing all three components: NVLink computing fabric for single computing domains, InfiniBand computing fabric, and Ethernet networking fabric, progressing rapidly with new switches, NICs, capabilities, and software stacks across all three. New CPUs, GPUs, networking NICs, and switches are on the way. What's exciting is that everything is compatible with CUDA and our comprehensive software stack. By investing in our software now, you can expect continuous improvements in performance. Similarly, investing in our architecture will ensure it integrates into more clouds and data centers seamlessly. The pace of innovation we are driving will enhance capabilities while reducing total cost of ownership. Our goal is to scale with the NVIDIA architecture for this new era of computing, ushering in an industrial revolution where we not only develop software but also manufacture artificial intelligence tokens at scale. Thank you.",
            "sentiment": "0.9"
        },
        {
            "speaker": "Operator",
            "title": "Operator",
            "content": "That will conclude our question-and-answer session and our call for today. We thank you all for joining and you may now disconnect.",
            "sentiment": "0.0"
        }
    ]
}